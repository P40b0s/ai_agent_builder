use std::io::Write;
use tokenizers::Tokenizer;

use candle_core::quantized::gguf_file;
use candle_core::{CudaDevice, Tensor};
use candle_core::{Device};
use candle_transformers::generation::{LogitsProcessor, Sampling};

use crate::{token_output_stream::TokenOutputStream};
use candle_transformers::models::quantized_qwen3::ModelWeights as Qwen;
//use candle_transformers::models::

//const DEFAULT_PROMPT: &str = "Write a Rust function to calculate the factorial of a given number.";

#[derive(Clone, Debug, Copy, PartialEq, Eq)]
enum ModelSize 
{
    /// 0.6b
    W3_0_6b,
    ///1.7b
    W3_1_7b,
    ///"4b
    W3_4b,
    ///8b
    W3_8b,
    ///14b
    W3_14b,
    ///32b
    W3_32b,
}

#[derive( Debug)]
struct ModelSettings
{
    /// GGUF file to load, typically a .gguf file generated by the quantize command from llama.cpp
    model: Option<String>,

    /// The initial prompt, use 'interactive' for entering multiple prompts in an interactive way
    /// and 'chat' for an interactive model where history of previous prompts and generated tokens
    /// is preserved.
    prompt: Option<String>,

    /// The length of the sample to generate (in tokens).
    sample_len: usize,

    /// The tokenizer config in json format.
    tokenizer: Option<String>,

    /// The temperature used to generate samples, use 0 for greedy sampling.
    temperature: f64,

    /// Nucleus sampling probability cutoff.
    top_p: Option<f64>,

    /// Only sample among the top K samples.
    top_k: Option<usize>,

    /// The seed to use when generating random samples.
    seed: u64,

    /// Enable tracing (generates a trace-timestamp.json file).
    tracing: bool,

    /// Process prompt elements separately.
    split_prompt: bool,

    /// Run on CPU rather than GPU even if a GPU is available.
    cpu: bool,

    /// Penalty to be applied for repeating tokens, 1. means no penalty.
    repeat_penalty: f32,

    /// The context size to consider for the repeat penalty.
    repeat_last_n: usize,

    /// The model size to use.
    size: ModelSize,
}
impl Default for ModelSettings
{
    fn default() -> Self 
    {
        ModelSettings
        {
            //model: Some("Qwen3-0.6B-Q4_K_M.gguf".to_owned()),
            model: Some("Qwen3-1.7B-Q8_0.gguf".to_owned()),
            prompt: None,
            sample_len: 1000,
            tokenizer: Some("tokenizer.json".to_owned()),
            temperature: 0.7,
            top_p: Some(0.8),
            top_k: Some(20),
            seed: 299792458,
            tracing: false,
            split_prompt: false,
            cpu: true,
            repeat_penalty: 1.5,
            repeat_last_n: 64,
            size: ModelSize::W3_0_6b
        }
    }
}

impl ModelSettings 
{
    fn tokenizer(&self) -> anyhow::Result<Tokenizer> 
    {
        let tokenizer_path = match &self.tokenizer 
        {
            Some(config) => std::path::PathBuf::from(config),
            None => 
            {
                let api = hf_hub::api::sync::Api::new()?;
                let repo = match self.size 
                {
                    ModelSize::W3_0_6b => "Qwen/Qwen3-0.6B",
                    ModelSize::W3_1_7b => "Qwen/Qwen3-1.7B",
                    ModelSize::W3_4b => "Qwen/Qwen3-4B",
                    ModelSize::W3_8b => "Qwen/Qwen3-8B",
                    ModelSize::W3_14b => "Qwen/Qwen3-14B",
                    ModelSize::W3_32b => "Qwen/Qwen3-32B",
                };
                let api = api.model(repo.to_string());
                api.get("tokenizer.json")?
            }
        };
        Tokenizer::from_file(tokenizer_path).map_err(anyhow::Error::msg)
    }

    fn model(&self) -> anyhow::Result<std::path::PathBuf> 
    {
        let model_path = match &self.model 
        {
            Some(config) => std::path::PathBuf::from(config),
            None => 
            {
                let (repo, filename, revision) = match self.size 
                {
                    ModelSize::W3_0_6b => ("unsloth/Qwen3-0.6B-GGUF", "Qwen3-0.6B-Q4_K_M.gguf", "main"),
                    ModelSize::W3_1_7b => ("unsloth/Qwen3-1.7B-GGUF", "Qwen3-1.7B-Q4_K_M.gguf", "main"),
                    ModelSize::W3_4b => ("unsloth/Qwen3-4B-GGUF", "Qwen3-4B-Q4_K_M.gguf", "main"),
                    ModelSize::W3_8b => ("unsloth/Qwen3-8B-GGUF", "Qwen3-8B-Q4_K_M.gguf", "main"),
                    ModelSize::W3_14b => ("unsloth/Qwen3-14B-GGUF", "Qwen3-14B-Q4_K_M.gguf", "main"),
                    ModelSize::W3_32b => ("unsloth/Qwen3-32B-GGUF", "Qwen3-32B-Q4_K_M.gguf", "main"),
                };
                let api = hf_hub::api::sync::Api::new()?;
                api.repo(hf_hub::Repo::with_revision(
                    repo.to_string(),
                    hf_hub::RepoType::Model,
                    revision.to_string(),
                ))
                .get(filename)?
            }
        };
        Ok(model_path)
    }
}

fn format_size(size_in_bytes: usize) -> String 
{
    if size_in_bytes < 1_000 
    {
        format!("{size_in_bytes}B")
    } 
    else if size_in_bytes < 1_000_000 
    {
        format!("{:.2}KB", size_in_bytes as f64 / 1e3)
    } 
    else if size_in_bytes < 1_000_000_000 
    {
        format!("{:.2}MB", size_in_bytes as f64 / 1e6)
    } 
    else 
    {
        format!("{:.2}GB", size_in_bytes as f64 / 1e9)
    }

}

pub struct OrfoModel
{
    model_settings: ModelSettings,
    model_weights: Qwen,
    tokenizer: Tokenizer,
    device: Device
}
impl OrfoModel
{
    pub fn load_model() -> anyhow::Result<Self> 
    {
        let model_settings = ModelSettings::default();
        println!(
        "avx: {}, neon: {}, simd128: {}, f16c: {}",
        candle_core::utils::with_avx(),
        candle_core::utils::with_neon(),
        candle_core::utils::with_simd128(),
        candle_core::utils::with_f16c()
    );
        println!(
            "temp: {:.2} repeat-penalty: {:.2} repeat-last-n: {}",
            model_settings.temperature, model_settings.repeat_penalty, model_settings.repeat_last_n
        );

        let model_path = model_settings.model()?;
        println!("filepath: {}", model_path.display());
        let mut file = std::fs::File::open(&model_path)?;
        let start = std::time::Instant::now();
        let device = Device::Cpu;
        let model = 
        {
            let model = gguf_file::Content::read(&mut file).map_err(|e| e.with_path(model_path))?;
            
            let mut total_size_in_bytes = 0;
            for (_, tensor) in model.tensor_infos.iter() 
            {
                let elem_count = tensor.shape.elem_count();
                total_size_in_bytes +=
                    elem_count * tensor.ggml_dtype.type_size() / tensor.ggml_dtype.block_size();
            }
            println!(
                "loaded {:?} tensors ({}) in {:.2}s",
                model.tensor_infos.len(),
                &format_size(total_size_in_bytes),
                start.elapsed().as_secs_f32(),
            );

            Qwen::from_gguf(model, &mut file, &device)?
        };
        println!("model built");

        let tokenizer = model_settings.tokenizer()?;
        
        Ok(OrfoModel 
        { 
            model_settings,
            model_weights: model,
            tokenizer,
            device
        })
    }
    
    pub fn prompt(&mut self, txt: &str) -> anyhow::Result<()>
    {
        let mut tos = TokenOutputStream::new(self.tokenizer.clone());
        //let prompt_str =  txt.to_owned();

        let prompt_str = format!("<|im_start|>system Ты корректор текстов на русском языке, находи ошибки в тексте, смысловых ошибок в тексте не будет, склонения слов не проверяй, возможны только орфографические ошибки. Если есть ошибки выдавай их в виде json объекта: {{ start_index: number, end_index: number, description: string }} где start_index начальный индекс слова с ошибкой измеряемый посимвольно с начала строки, end_index конечный индекс слова с ошибкой измеряемый посимвольно с начала строки, description описание ошибки, если ошибок несколько выводи как массив объектов. Если ошибок нет, не выводи ничего.<|im_end|>
        <|im_start|>user\n{txt}<|im_end|>\n<|im_start|>assistant\n");
        //print!("formatted prompt: {}", &prompt_str);

        let tokens = tos
            .tokenizer()
            .encode(prompt_str, true)
            .map_err(anyhow::Error::msg)?;

        let tokens = tokens.get_ids();

        let to_sample = self.model_settings.sample_len.saturating_sub(1);

        let mut all_tokens = vec![];

        let mut logits_processor = 
        {
            let temperature = self.model_settings.temperature;
            let sampling = if temperature <= 0. 
            {
                Sampling::ArgMax
            } 
            else 
            {
                match (self.model_settings.top_k, self.model_settings.top_p) 
                {
                    (None, None) => Sampling::All { temperature },
                    (Some(k), None) => Sampling::TopK { k, temperature },
                    (None, Some(p)) => Sampling::TopP { p, temperature },
                    (Some(k), Some(p)) => Sampling::TopKThenTopP { k, p, temperature },
                }
            };
            LogitsProcessor::from_sampling(self.model_settings.seed, sampling)
        };

        let start_prompt_processing = std::time::Instant::now();

        let mut next_token = if !self.model_settings.split_prompt 
        {
            let input = Tensor::new(tokens, &self.device)?.unsqueeze(0)?;
            let logits = self.model_weights.forward(&input, 0)?;
            let logits = logits.squeeze(0)?;
            logits_processor.sample(&logits)?
        } 
        else 
        {
            let mut next_token = 0;
            for (pos, token) in tokens.iter().enumerate() 
            {
                let input = Tensor::new(&[*token], &self.device)?.unsqueeze(0)?;
                let logits = self.model_weights.forward(&input, pos)?;
                let logits = logits.squeeze(0)?;
                next_token = logits_processor.sample(&logits)?
            }
            next_token
        };

        let prompt_dt = start_prompt_processing.elapsed();

        all_tokens.push(next_token);

        if let Some(t) = tos.next_token(next_token)? 
        {
            print!("{t}");
            std::io::stdout().flush()?;
        }

        let eos_token = *tos.tokenizer().get_vocab(true).get("<|im_end|>").unwrap();

        let start_post_prompt = std::time::Instant::now();

        let mut sampled = 0;
        for index in 0..to_sample 
        {
            let input = Tensor::new(&[next_token], &self.device)?.unsqueeze(0)?;
            let logits = self.model_weights.forward(&input, tokens.len() + index)?;
            let logits = logits.squeeze(0)?;
            let logits = if self.model_settings.repeat_penalty == 1. 
            {
                logits
            } 
            else 
            {
                let start_at = all_tokens.len().saturating_sub(self.model_settings.repeat_last_n);
                candle_transformers::utils::apply_repeat_penalty(
                    &logits,
                    self.model_settings.repeat_penalty,
                    &all_tokens[start_at..],
                )?
            };
            next_token = logits_processor.sample(&logits)?;
            all_tokens.push(next_token);
            if let Some(t) = tos.next_token(next_token)? 
            {
                print!("{t}");
                std::io::stdout().flush()?;
            }
            sampled += 1;
            if next_token == eos_token 
            {
                break;
            };
        }

        if let Some(rest) = tos.decode_rest().map_err(candle_core::Error::msg)? 
        {
            print!("{rest}");
        }

        std::io::stdout().flush()?;
        let dt = start_post_prompt.elapsed();
        println!(
            "\n\n{:4} prompt tokens processed: {:.2} token/s",
            tokens.len(),
            tokens.len() as f64 / prompt_dt.as_secs_f64(),
        );
        println!(
            "{sampled:4} tokens generated: {:.2} token/s",
            sampled as f64 / dt.as_secs_f64(),
        );
        Ok(())
    }


}
// pub fn run_model() -> anyhow::Result<()> 
// {
//     let model_settings = ModelSettings::default();
//     println!(
//         "avx: {}, neon: {}, simd128: {}, f16c: {}",
//         candle_core::utils::with_avx(),
//         candle_core::utils::with_neon(),
//         candle_core::utils::with_simd128(),
//         candle_core::utils::with_f16c()
//     );
//     println!(
//         "temp: {:.2} repeat-penalty: {:.2} repeat-last-n: {}",
//         model_settings.temperature, model_settings.repeat_penalty, model_settings.repeat_last_n
//     );

//     let model_path = model_settings.model()?;
//     println!("filepath: {}", model_path.display());
//     let mut file = std::fs::File::open(&model_path)?;
//     let start = std::time::Instant::now();
//     let device = Device::Cpu;

//     let mut model = 
//     {
//         let model = gguf_file::Content::read(&mut file).map_err(|e| e.with_path(model_path))?;
//         let mut total_size_in_bytes = 0;
//         for (_, tensor) in model.tensor_infos.iter() 
//         {
//             let elem_count = tensor.shape.elem_count();
//             total_size_in_bytes +=
//                 elem_count * tensor.ggml_dtype.type_size() / tensor.ggml_dtype.block_size();
//         }
//         println!(
//             "loaded {:?} tensors ({}) in {:.2}s",
//             model.tensor_infos.len(),
//             &format_size(total_size_in_bytes),
//             start.elapsed().as_secs_f32(),
//         );
//         Qwen::from_gguf(model, &mut file, &device)?
//     };
//     println!("model built");

//     let tokenizer = model_settings.tokenizer()?;
//     let mut tos = TokenOutputStream::new(tokenizer.clone());
//     let prompt_str = model_settings
//         .prompt
//         .clone()
//         .unwrap_or_else(|| DEFAULT_PROMPT.to_string());

//     let prompt_str = format!("<|im_start|>user\n{prompt_str}<|im_end|>\n<|im_start|>assistant\n");
//     print!("formatted prompt: {}", &prompt_str);

//     let tokens = tos
//         .tokenizer()
//         .encode(prompt_str, true)
//         .map_err(anyhow::Error::msg)?;

//     let tokens = tokens.get_ids();

//     let to_sample = model_settings.sample_len.saturating_sub(1);

//     let mut all_tokens = vec![];

//     let mut logits_processor = 
//     {
//         let temperature = model_settings.temperature;
//         let sampling = if temperature <= 0. 
//         {
//             Sampling::ArgMax
//         } 
//         else 
//         {
//             match (model_settings.top_k, model_settings.top_p) {
//                 (None, None) => Sampling::All { temperature },
//                 (Some(k), None) => Sampling::TopK { k, temperature },
//                 (None, Some(p)) => Sampling::TopP { p, temperature },
//                 (Some(k), Some(p)) => Sampling::TopKThenTopP { k, p, temperature },
//             }
//         };
//         LogitsProcessor::from_sampling(model_settings.seed, sampling)
//     };

//     let start_prompt_processing = std::time::Instant::now();

//     let mut next_token = if !model_settings.split_prompt 
//     {
//         let input = Tensor::new(tokens, &device)?.unsqueeze(0)?;
//         let logits = model.forward(&input, 0)?;
//         let logits = logits.squeeze(0)?;
//         logits_processor.sample(&logits)?
//     } 
//     else 
//     {
//         let mut next_token = 0;
//         for (pos, token) in tokens.iter().enumerate() 
//         {
//             let input = Tensor::new(&[*token], &device)?.unsqueeze(0)?;
//             let logits = model.forward(&input, pos)?;
//             let logits = logits.squeeze(0)?;
//             next_token = logits_processor.sample(&logits)?
//         }
//         next_token
//     };

//     let prompt_dt = start_prompt_processing.elapsed();

//     all_tokens.push(next_token);

//     if let Some(t) = tos.next_token(next_token)? 
//     {
//         print!("{t}");
//         std::io::stdout().flush()?;
//     }

//     let eos_token = *tos.tokenizer().get_vocab(true).get("<|im_end|>").unwrap();

//     let start_post_prompt = std::time::Instant::now();

//     let mut sampled = 0;
//     for index in 0..to_sample 
//     {
//         let input = Tensor::new(&[next_token], &device)?.unsqueeze(0)?;
//         let logits = model.forward(&input, tokens.len() + index)?;
//         let logits = logits.squeeze(0)?;
//         let logits = if model_settings.repeat_penalty == 1. 
//         {
//             logits
//         } 
//         else 
//         {
//             let start_at = all_tokens.len().saturating_sub(model_settings.repeat_last_n);
//             candle_transformers::utils::apply_repeat_penalty(
//                 &logits,
//                 model_settings.repeat_penalty,
//                 &all_tokens[start_at..],
//             )?
//         };
//         next_token = logits_processor.sample(&logits)?;
//         all_tokens.push(next_token);
//         if let Some(t) = tos.next_token(next_token)? 
//         {
//             print!("{t}");
//             std::io::stdout().flush()?;
//         }
//         sampled += 1;
//         if next_token == eos_token 
//         {
//             break;
//         };
//     }

//     if let Some(rest) = tos.decode_rest().map_err(candle_core::Error::msg)? 
//     {
//         print!("{rest}");
//     }

//     std::io::stdout().flush()?;
//     let dt = start_post_prompt.elapsed();
//     println!(
//         "\n\n{:4} prompt tokens processed: {:.2} token/s",
//         tokens.len(),
//         tokens.len() as f64 / prompt_dt.as_secs_f64(),
//     );
//     println!(
//         "{sampled:4} tokens generated: {:.2} token/s",
//         sampled as f64 / dt.as_secs_f64(),
//     );
//     Ok(())
// }